<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>My Audio Samples</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #f4f4f4;
        }
        .container {
            max-width: 800px;
            margin: auto;
            background: #fff;
            padding: 20px;
            border-radius: 5px;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
        }
        h1, h2 {
            text-align: center;
        }
        audio {
            display: block;
            margin: 20px auto;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Audio Samples</h1>
        <p>
            Welcome to my audio samples page! Below is a collection of audio examples generated from my recent work. 
            These samples demonstrate various techniques and models applied to text-to-speech synthesis.
        </p>

        <h2>Abstract</h2>
        <p>
            Fast and efficient text-to-speech (TTS) synthesis for device-side applications is increasingly critical, 
            yet current models often face a trade-off between generation speed and audio quality. 
            Although Diffusion models can produce high-quality audio, they require numerous sampling steps, 
            which makes them less suitable for real-time applications. On the other hand, flow-matching models promise reduced sampling steps 
            but often need more robust representation learning, which often results in suboptimal synthesis quality. 
            Thus, We introduce ContrastiveFlowTTS (CFTTS), a novel flow-based TTS model that addresses these challenges by enhancing representation learning by integrating two self-supervised contrastive methods. Specifically, we incorporate a HuBERT-based loss to align text encoder outputs with speech representations and leverage contrastive predictive coding (CPC) to extract deep, meaningful features. Our approach achieves high-quality, low-latency speech synthesis compared to other diffusion-based TTS models.
        </p>

        <h2>Audio Samples</h2>
        
        <!-- Example Audio 1 -->
        <h3>Sample 1: Model A Output</h3>
        <audio controls>
            <source src="audio/sample1.mp3" type="audio/mpeg">
            Your browser does not support the audio element.
        </audio>

        <!-- Example Audio 2 -->
        <h3>Sample 2: Model B Output</h3>
        <audio controls>
            <source src="audio/sample2.mp3" type="audio/mpeg">
            Your browser does not support the audio element.
        </audio>

        <!-- Example Audio 3 -->
        <h3>Sample 3: Model C Output</h3>
        <audio controls>
            <source src="audio/sample3.mp3" type="audio/mpeg">
            Your browser does not support the audio element.
        </audio>
    </div>
</body>
</html>
